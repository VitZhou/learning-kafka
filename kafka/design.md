# 设计
### 动机
我们设计的kafka能够作为统一的平台来处理大型公司可能拥有的所有实时数据.为了做到这一点,我们不得不考虑一系列相当广泛的用例.

它必须具有高吞吐量来支持大量事件流,例如实时日志聚合.

它必须能够优雅的处理大量的积压数据,以便能够支持从离线系统定期进行数据加载.

这也意味着系统将不得不通过低延迟传递,来处理更传统的消息传递用例.

我们希望支持:Feed分区,分布式和实时处理.以创建新的派生Feed.这刺激了我们的分区和消费者模式.

最后,在将流馈送到其他数据系统进行服务的情况下,我们知道系统必须能够在机器故障的情况下保证容错.

为了支持这些使用导致我们进行了一系列独特的元素的设计,比传统的消息系统更像数据库日志.我们将在以下部分中概述设计的一些要素.

### 持久性
==Don't fear the filesystem!==
Kafka在很大程度上依赖文件系统来存储和缓存消息.有一个普遍的看法:“磁盘缓慢”,这使人们怀疑持久结构可以提供有竞争力的性能.磁盘可以说是既很慢又很快,事实上磁盘的快慢取决于对它们的使用方式,正确设计的磁盘结构通常可以与网络一样快.

关于磁盘性能的关键事实是,磁盘驱动的吞吐量与十年前的磁盘吞吐量今非昔比.使用六个7200rpm STAT RAID-5阵列的JBOD配置的线性写入性能约为600M/秒,但是随机写入的性能大约只有100k/秒,有将近6000倍的性能差异.这些线性读取和写入是所有使用模式中最可预测的,并且由操作系统进行了大量优化.现代操作系统提供预读（read-ahead）和后写(write-behind)技术.预读就是提前将一个比较大的磁盘块中内容读入内存，后写是将一些较小的逻辑写入操作合并起来组成比较大的物理写入操作.在此[ACM队列文章](http://queue.acm.org/detail.cfm?id=1563874)中可以找到有关此问题的进一步讨论; 他们发现实际上顺序磁盘访问在某些情况下比随机内存访问速度更快！

![](img/jacobs3.jpg)

为了弥补这种性能差异,现代操作系统在使用主存储器进行磁盘缓存时变得越来越激进.现代操作系统很乐意将所有的可用内存转做磁盘缓存,即时在需要回收这些内存的情况下会付出一些性能方面的代价.所有磁盘的读写都要经过这个统一缓存.想要舍弃这个特性都不太容易，除非使用直接I/O,因此，对于一个进程而言，即使它在进程内的缓存中保存了一份数据，这些数据也很可能会在操作系统的pagecache中重复,从而有效的存储两次.

更进一步讲，我们是在JVM的基础之上开发的系统，只要是了解过一些Java中内存使用方法的人都知道这两点：
- java对象内存的开销非常大,通常是存储对象大小的几倍(甚至更高)
- 随着堆内数据的增加,java的垃圾回收机制会变得越来越不明确，回收所花费的代价也会越来越大.

由于以上因数,使用文件系统并依赖于pagecache的方案优于维护内存中缓存和其他结构的方案----我们至少可以通过访问(挪用)所有空闲内存来使缓存可用大小增加一倍,然后通过保存压缩后的字节结构而非单个对象,来使得在32G内存的机器上可以达到高达28-30G的缓存,并且没有GC的性能消耗.此外,即便重启系统,该缓存仍然保持有效,而进程内缓存需要进行缓存重建(对于10GB缓存可能需要10分钟时间).否则它将需要启动一个完全冷却的缓存(这意味着低得可怜的初始性能). 这种方案也大大的简化了代码,因为在缓存和文件系统之间保持一致性的所有逻辑现在都在操作系统中,这往往比一次性进程中的尝试更有效和更正确的执行.如果您的磁盘使用量有利于线性读取,则预读将有效地预填充此缓存,并在每个磁盘读取时使用有用的数据.

这就让人联想到一个非常简单的设计方案:不是要在内存中保存尽可能多的数据并在需要时将这些数据刷新（flush）到文件系统，而是我们要做完全相反的事情.所有数据都要立即写入文件系统中持久化的日志中但不进行刷新数据的任何调用.实际上数据只是被转移到内核的pageche中,随后操作系统内核将这些数据刷新到磁盘中.

这种以页面缓存为中心的设计风格在一篇讲解[Varnish](http://varnish-cache.org/docs/trunk/phk/notes.html)的设计思想的文章中有详细的描述（文风略带有助于身心健康的傲气）。

### 常量时长足矣
在消息系统中使用的持久数据结构通常是具有相关联的BTree或其他通用随机访问数据结构的消费者队列,以维护关于消息的元数据.BTree是通用的可用数据结构,可以在消息系统中支持各种各样的事务和非事务性语义,它的确也带来了一个非常高的处理开销，Btree运算的时间复杂度为O(log N)。一般O(log N)被认为基本上等于常量时长.但是磁盘操作不是这样,磁盘寻道时间一次就需要10ms,每个磁盘在同一时间内只能做一次寻道操作,所以并行性有限.因此,即便是少量的查找也会需要非常大的时间开销.因为存储系统混合了高速缓存操作和真正的物理磁盘操作,所以树型结构（tree structure）可观察到的性能往往是超线性的（superlinear）,数据吞吐量随着内存的增加而提高,如果数据增加两倍,那么性能的下降不止两倍.

直觉上讲，持久化队列可以按照通常的日志解决方案的样子构建，只是简单的文件读取和简单地向文件中添加内容.这种结构的优点是所有操作都是O（1）,并且读取和写入不会阻塞彼此.这具有非常明显的性能优势,因为性能与数据大小完全分离----一个服务器现在可以充分利用一些便宜的,低转速的,容量超过1TB的SATA驱动器.虽然这些驱动器寻道操作的性能很低,但是对于大量的读写而言,这些驱动器具有可接受的性能.而只需1/3的价格就能获得3倍的容量

无需任何性能损失就可以访问几乎无限的磁盘空间,这意味着我们可以提供消息传递系统中通常没有的功能.例如,在kafka中,我们可以保留消息相对较长的时间（例如一个星期）,而不是消息一旦被消费删除. 这将为消费者带来很大的灵活性,正如我们将要描述的那样.

### 效率
我们在效率上付出了很大的努力.我们的主要用例之一是处理Web活动数据,这是非常高的数量:每个页面视图可能会生成数十个写入.此外，我们假设每个发布的消息至少由一个消费者（通常很多）读取.因而我们要为消息使用而不是消息的产生进行系统优化.

我们建设和运行多个类似系统的经验中发现,效率是有效的多租户业务的关键.如果下游基础设施服务可能由于应用程序在使用中的一个很小的突变而地成为瓶颈,则这种小的改变通常会产生问题.通过非常快的速度，我们帮助确保应用程序在基础设施之前的负载下翻腾。当在集中式集群中运行支持数十个或数百个应用程序的集中式服务时，这一点特别重要，因为使用模式的变化是近乎每日的事件。

我们上一节中讨论了磁盘的效率.一旦不良的磁盘访问模式被消除,这种类型的系统有两个低效率的常见原因:太小的I/O操作和过多的字节复制.

发生小的I/O问题在客户端和服务器之间以及服务器自己的持久性操作中都很常见.

为了避免这种情况,我们的协议是围绕一个自然地将消息组合在一起的“消息集”（message set）抽象概念构建的.这允许网络请求将消息分组在一起.消息集将消息进行自然分组.这么做能让网络请求把消息合成一个小组,分摊网络往返（roundtrip）所带来的开销，而不是每次仅仅发送一个单个消息.服务器依次将大块消息追加到其日志中,消费者一次获取大的线性块.

这种简单的优化产生了指数级的性能加速.批处理导致更大的网络数据包,更大的顺序磁盘操作,连续的内存块等.这些都允许Kafka将随机写入的突发流转换成消费者的线性写入流.

另一个低效率是字节复制.在低负载下,这不是一个问题,但负载影响是显着的.为了避免这种情况,我们采用由生产者,broker和消费者共享的标准化二进制消息格式（因此数据块可以在它们之间进行转换）.

使用共通的格式后就能对最重要的操作进行优化了：持久化后日志块（chuck）的网络传输。为了将数据从页面缓存直接传送给socket，现代的Unix操作系统提供了一个高度优化的代码路径（code path）。在Linux中这是通过[sendfile系统调用](http://man7.org/linux/man-pages/man2/sendfile.2.html)实现的.

为了理解sendfile所带来的效果，重要的是要理解将数据从文件传输到socket的数据路径：
- 操作系统将数据从磁盘中读取到内核空间里的页面缓存(pageche)
- 应用程序将数据从内核空间读入到用户空间的缓冲区
- 应用程序将读到的数据写回内核空间并放入socke的缓冲区
- 操作系统将数据从socket的缓冲区拷贝到NIC（网络借口卡，即网卡）的缓冲区，自此数据才能通过网络发送出去

这样效率显然很低，因为里面涉及4次拷贝,2次系统调用.使用sendfile就可以避免这些重复的拷贝操作,让操作系统直接将数据从页面缓存发送到网络中,其中只需最后一步中的将数据拷贝到NIC的缓冲区.

我们预期的一种常见的用例是一个话题拥有多个消息使用者。采用前文所述的零拷贝优化方案，数据只需拷贝到页面缓存中一次，然后每次发送给使用者时都对其进行重复使用即可，而无须先保存到内存中，然后在阅读该消息时每次都需要将其拷贝到内核空间中。如此一来，消息使用的速度就能接近网络连接的极限。

这种pagecache和sendfile的组合意味着在消费者大多被抓住的Kafka群集上，您将看到磁盘上没有任何读取活动,因为它们将完全从缓存中提供数据.

有关Java中的sendfile和零拷贝支持的更多背景信息，请参阅这篇[文章](http://www.ibm.com/developerworks/linux/library/j-zerocopy).

### 端到端的批量压缩
在某些情况下,瓶颈实际上不是CPU或磁盘,而是网络带宽. 这一点对于需要将消息在个数据中心间进行传输的数据管道来说,尤其如此.当然,无需来自Kafka的支持,用户总是可以自行将消息压缩后进行传输,但这么做的压缩率会非常低,因为不同的消息里都有很多重复性的内容（比如JSON里的字段名,web日志中的用户代理或者常用的字符串）.高效压缩需要将多条消息一起进行压缩而不是分别压缩每条消息.

通过运行递归消息集,Kafka对这种压缩方式提供了支持.一批消息可以打包到一起进行压缩,然后以这种形式发送给服务器.这批消息都会被发送给同一个消息使用者,并会在到达使用者那里之前一直保持为被压缩的形式.

Kafka支持GZIP和Snappy压缩协议.关于压缩的更多更详细的信息,请参见[这里](https://cwiki.apache.org/confluence/display/KAFKA/Compression).

### 生产者
##### 负载均衡
生产者将数据直接发送到作为分区的leader的broker,而不需要任何中间路由层.为了达到这个目的,所有kafka的节点都有健康检查接口,而且topic的分区的leader在任何时间都允许生产者直接请求.

客户端控制其发布消息的分区,可以使用随机负载完成,也可以通过一些语义分割功能完成,我们通过允许用户指定一个分区的key并将其用于散列到一个分区（也可以选择覆盖分区函数，如果需要）,我们公开语义分区的接口.例如,如果用User id做可以,那么所有user的数据都会发布到同一个分区,反过来又使消费者对其消费做出局部假设,这种分区风格被明确地设计允许消费者对局部敏感的的处理.
##### 异步发送
批处理是效率的主要推动力之一,为了使其批量生产kafka生产者能够在内存中累积数据,并在一个请求中发送更大批量的数据.批处理的最大累积信息数量和最大等待时间可以配置（例如64k或10ms）.这允许多个字节的累积发送,这样可以利用服务器端的少数的大I/O操作来发送更多的累积信息.这种缓冲是可配置的，并且提供了一种机制来折算少量额外的延迟以获得更好的吞吐量。

有关配置和生产者的API的详细信息可以在配置目录中找到

### 消费者
kafka消费者通过向其想要消费的leader分区的broker发起"fetch"请求来工作.消费者在每个请求中指定其在日志中的偏移量,并从该位置返回一大堆日志.因此,消费者对此位置具有重要的控制权,并且可以将其倒回以在需要时重新使用数据.

我先考虑一个简单的问题:到底应该是消费者去broker拉取数据还是应该由broker将数据推送给消费者.和大部分消息系统一样,Kafka在这方面遵循了一种更加传统的设计思路：由生产者将数据Push给broker,然后由消费者将数据从broker那里Pull回来.近来有些系统，比如scribe和flume，更着重于日志统计功能，遵循了一种非常不同的基于Push的设计思路，其中每个节点都可以作为broker，数据一直都是向下游Push的。上述两种方法都各有优缺点。然而，因为基于Push的系统中broker控制着数据的传输速率，因此它难以应付大量不同种类的消费者。我们的设计目标是，让消费者能以它最大的速率使用数据.不幸的是，在Push系统中当数据的使用速率低于产生的速率时，消费者往往会处于超载状态（这实际上就是一种拒绝服务攻击）。基于Pull的系统在消费者的处理速度稍稍落后于broker的情况下会表现更佳，而且还可以让使用者在有能力的时候往往前赶赶。让使用者采用某种退避协议（backoff protocol）向代理表明自己处于超载状态，可以解决部分问题，但是，将传输速率调整到正好可以完全利用（但从不能过度利用）使用者的处理能力可比初看上去难多了。以前我们尝试过多次，想按这种方式构建系统，得到的经验教训使得我们选择了更加常规的Pull模型。

基于以拉为主的系统的另一个优点是:适合将批量数据发送给消费者处理.基于push的系统必须选择立即发送请求或累积更多数据,然后再发送,而不必知道下游用户是否能够立即处理它.如果调整为低延迟,还可能会造成缓存还没结束时就传输了单条数据过去,这是非常浪费的.基于拉式的设计会修复此问题,因为消费者总是在日志中的当前位置拉出之后所有可用的消息（或配置）.这样可以在不引入不必要的延迟的情况下,可以获得比较好的批处理能力.

基于pull为主的系统的坏处在于:如果broker中的数据是空的,那么消费者就需要循环检查,使用空轮询的繁忙检查方式等待数据的到来.为了解决这个问题,我们在我们的拉取请求中设置参数,允许消费者请求在“长轮询(long poll)”时阻塞等待,直到数据到达.

你可以想象一些其他从端到端的一些可能性设计.生产者把记录写入到本地日志中.服务器将从消费者拉取的数据中拉取.一种类似的存储和转发的生产者模型经常被提议.这虽然挺有趣的,但不合适有成千上万生产者的情况.我们在大规模运行数据存储系统的经验来看,成千上万的磁盘跨越多个应用并不让系统更可靠,操作起来将会是一个噩梦.在实践中,我们发现可以创建具有很强壮的SLAS保障的,大规模的管道,并不需要提供者有持久化能力.

##### 消费者跟踪
跟踪已消费内容是信息传递系统的关键性能之一.

大部分的信息系统在服务器端记录哪些信息被消费的元数据信息.也就是说,消息被传送给消费者时,服务器要么在本地马上记录日志,要么等待消费者反馈后记录.这样的话相当不直观.事实上,对一台服务器,很难理清除这个状态去了哪.因为大部分消息存储系统中,数据结构很难被扩展.这也依赖于编程的语义.如果服务器知道消息被消费后可以马上删除,那么就可以维持比较小的数据集.

不巧的是,让服务器和消费者对已经消费的数据达成一致并不是一件简单的事.如果服务器在每次数据分发出去后,马上标记消息已经被消费,如果消费者处理消息失败了(例如宕机了),那么消息能会丢失.为了解决这个问题.很多消息系统添加了反馈机制.用于标记已经被发送,而不是被消费.服务器等待消费者发送一个反馈来确认消息已经被消费.这个策略解决消息丢失的问题.但是同时也引发新的问题.首先,如果消费已消费了记录,但是在反馈时失效,则有可能消费了两次.其次一个是多了一个来回的性能消耗.现在服务器就要为每个消息保存不同的状态(先锁定,这样不会发送第二次,然后标记为永久消费后才能把它删除).还有些麻烦的问题需要处理,比如消息被发送了,但是从来没有接受到反馈.

Kafka使用不一样的处理方式,topic被划分成一系列有序的分区集合,每个分区在一个时刻仅被订阅分组中的一个消费者消费.这意味者每个消费者在一个分区的位置就是只是一个数值.用于记录下一次消息要被消费的位置.这意味着记录消费者状态的大地吗非常小,只是每个分区一个数值.这个状态可以定期做检查点,这使等价的消息反馈代价非常小.

这个方案的另外一个好处是,消费者可以优雅的重新指定一个旧的位移位置,并重新消费数据.这个通常和队列观念有点相悖,但是对很多消费者来说是一个很重的特性.例如,如果消费代码有bug,并且在一些消息被消费后,一旦bug被修复,消费者可以重新使用这些消息.

##### 离线数据加载
可扩展的持久性存储能力,使得消费者能定期批量的把数据导入到离线系统中,如hadoop或关系数据库.

### 消息可靠新保证
生产者与消费者之间的语义保证可以有以下几种方式提供:
- At most once: 消息可能会丢，但绝不会重复传输
- At least one: 消息绝不会丢，但可能会重复传输
- Exactly once: 每条消息肯定会被传输一次且仅传输一次，很多时候这是用户所想要的

其实际上是需要解决两个问题:发布消息时的持久性保证,和消费消息时的保证.

许多系统声称提供“正好一次”发送语义，声称的很好，但是大部分这些说法都是误导（即：他们没解释消费者和生产者可能失败的情况。如多个消费处理，数据写入磁盘丢失的情况）。

